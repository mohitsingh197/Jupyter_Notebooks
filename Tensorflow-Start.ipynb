{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "!pip install --upgrade tensorflow", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom functools import partial\ntf.__version__", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "'1.3.0'"
                    }, 
                    "execution_count": 2, 
                    "metadata": {}
                }
            ], 
            "execution_count": 2
        }, 
        {
            "source": "a = tf.Variable(10)\nb = tf.Variable(20)\nf = a * b + 100\n\nsess = tf.Session()\nsess.run(a.initializer)\nsess.run(b.initializer)\nsess.run(f)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "with tf.Session() as sess:\n    a.initializer.run()\n    b.initializer.run()\n    result = f.eval()\nresult", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "init = tf.global_variables_initializer()\nsess = tf.InteractiveSession()\ninit.run()\nresult = f.eval()\nresult", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "a.graph is tf.get_default_graph()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "A = tf.placeholder(tf.float32, shape=(None, 3))\nC = tf.placeholder(tf.float32, shape=(None, 3))\nB = A * C\nwith tf.Session() as sess:\n    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]], C:[[1,4,9]]})\n    B_val_2 = B.eval(feed_dict={A: [[4,5,6],[7,8,9]], C:[[4,5,6],[7,8,9]]})\nprint(B_val_1)\nprint(B_val_2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# Tensorflow Steps", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#Define placeholders\nX = tf.placeholder(tf.float32, shape= , name='X')\ny = tf.placeholder(tf.int32, shape= , name='y')\n\n#Batch Normalization partial function\ntraining = True\nbct_normalization_step = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n\n###---------------------------- l1/l2 Regularization Starts ----------------------------###\n\n#Define dense layer with regularization partial function\nscale = 0.01\ndns_layer = partial(tf.layers.dense, \\\n                    kernal_regularizer=tf.contribi.layers.l1_regularizer(scale))# tf.contribi.layers.l1_regularizer(scale)/tf.contribi.layers.l1_l2_regularizer(scale)\n\n#Define layers\nhidden_1 = dns_layer(input, n_inputs)\nbn1 = tf.nn.elu(bct_normalization_step(hidden1)) #batch normalization with activation\nhidden2 = dns_layer(bn1, n_hidden1)\nbn2 = tf.nn.elu(bct_normalization_step(hidden2)) #batch normalization with activation\nhidden3 = dns_layer(bn2, n_hidden2)\nbn3 = tf.nn.elu(bct_normalization_step(hidden3)) #batch normalization with activation\nlogits_b4_bn = dns_layer(bn3, n_outputs)\nlogits = tf.nn.elu(logits_b4_bn) #batch normalization with activation\n\n#regularization losses\nreg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n#Define loss\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y) # Same as applyiing softmax activation and then calculating cross entropy\nbase_loss = tf.reduce_mean(xentropy, name='loss')\n\nloss = tf.add_n([base_loss] + reg_losses, name='loss')\n\n###------------------------- l1/l2 Regularization Ends -----------------------------------###\n\n#OR\n\n###---------------------------- Dropout Starts --------------------------------------###\n\ndropout_rate = 0.5 # == 1 - keep_prob\nX_drop = tf.layers.dropout(input, dropout_rate, training=training)\n\nhidden1 = tf.layers.dense(X_drop, n_inputs)\nbn1 = tf.nn.elu(bct_normalization_step(hidden1)) #batch normalization with activation\nhidden1_drop = tf.layers.dropout(bn1, dropout_rate, training=training)\n\nhidden2 = tf.layers.dense(hidden1_drop, n_hidden1)\nbn2 = tf.nn.elu(bct_normalization_step(hidden2)) #batch normalization with activation\nhidden2_drop = tf.layers.dropout(bn2, dropout_rate, training=training)\n\nhidden3 = tf.layers.dense(hidden2_drop, n_hidden2)\nbn3 = tf.nn.elu(bct_normalization_step(hidden3)) #batch normalization with activation\nhidden3_drop = tf.layers.dropout(bn3, dropout_rate, training=training)\n\nlogits_b4_bn = dns_layer(hidden3_drop, n_outputs)\nlogits = tf.nn.elu(logits_b4_bn) #batch normalization with activation\n\n\n### ------------ ------------------- Dropout Ends ----------------------------------------###\n\nlearning_rate = 0.01\n\n#Define optimizer\noptimizer = tf.train.MomentumOptimizer(learning_rate, use_nesterov=True) # OR tf.train.GradientDescentOptimizer(learning_rate)\ntraining_op = optimizer.minimize(loss)\n\n#How to evaluate\ncorrect = tf.nn.in_top_k(logits, y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\n#Variable initializer\ninit = tf.global_variables_initializer()\ntf.train.Saver()\n\n#Initialize epochs and batchSize\nn_epochs = 40\nbatch_size= 64\n\nwith tf.Sessions() as sess:\n    init.run() #initializes the variables\n    for epoch in range n_outputs:\n        for iteration in range (m // batch_size):\n            x_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: x_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: x_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict = {})\n        \n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# DNNClassifier ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)\nprint(y_train.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "x_train = x_train.reshape(60000, 784)\n#y_train = pd.get_dummies(y_train)\nprint(x_train.shape)\nprint(y_train.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from sklearn.preprocessing import MinMaxScaler", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "scalar = MinMaxScaler(feature_range=(0, 1))\nscalar.fit(x_train)\nx_train = scalar.transform(x_train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "y_train_int32 = np.asarray(y_train, dtype=np.int32)\nprint(y_train_int32.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(x_train.shape)\nprint(y_train.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(x_train)\ndnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=feature_cols)\ndnn_clf = tf.contrib.learn.SKCompat(dnn_clf)\ndnn_clf.fit(x_train, y_train, batch_size = 50, steps = 40000)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "feature_cols", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def foo():\n  with tf.variable_scope(\"foo\", reuse=True):\n    v = tf.get_variable(\"v\", [1])\n  return v", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "v1 = foo()  # Creates v.\nv2 = foo()  # Gets the same, existing v.\nprint(v1 == v2)\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    init.run()\n    print(v1.eval())\n    print(v1.assign(2.0, validate_shape=False))\n    print(v1.eval())\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}