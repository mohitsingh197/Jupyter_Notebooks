{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "!pip install --upgrade tensorflow", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "import tensorflow as tf\nimport pandas as pd\nimport numpy as np\ntf.__version__", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "a = tf.Variable(10)\nb = tf.Variable(20)\nf = a * b + 100\n\nsess = tf.Session()\nsess.run(a.initializer)\nsess.run(b.initializer)\nsess.run(f)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "with tf.Session() as sess:\n    a.initializer.run()\n    b.initializer.run()\n    result = f.eval()\nresult", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "init = tf.global_variables_initializer()\nsess = tf.InteractiveSession()\ninit.run()\nresult = f.eval()\nresult", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "a.graph is tf.get_default_graph()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "A = tf.placeholder(tf.float32, shape=(None, 3))\nC = tf.placeholder(tf.float32, shape=(None, 3))\nB = A * C\nwith tf.Session() as sess:\n    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]], C:[[1,4,9]]})\n    B_val_2 = B.eval(feed_dict={A: [[4,5,6],[7,8,9]], C:[[4,5,6],[7,8,9]]})\nprint(B_val_1)\nprint(B_val_2)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# Tensorflow Steps", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#Define placeholders\nX = tf.placeholder(tf.float32, shape= , name='X')\ny = tf.placeholder(tf.int32, shape= , name='y')\n\n#Define layers\nhidden1 = tf.layers.dense(input, n_inputs, activation=tf.nn.relu)\nhidden2 = tf.layers.dense(hidden1, n_hidden1, activation=tf.nn.relu)\nhidden3 = tf.layers.dense(hidden2, n_hidden2, activation=tf.nn.relu)\nlogits = tf.layers.dense(hidden3, n_outputs)\n\n#Define loss\nxentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y) # Same as applyiing softmax activation and then calculating cross entropy\nloss = tf.reduce_mean(xentropy, name='loss')\n\nlearning_rate = 0.01\n\n#Define optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntraining_op = optimizer.minimize(loss)\n\n#How to evaluate\ncorrect = tf.nn.in_top_k(logits, y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\n#Variable initializer\ninit = tf.global_variables_initializer()\ntf.train.Saver()\n\n#Initialize epochs and batchSize\nn_epochs = 40\nbatch_size= 64\n\nwith tf.Sessions() as sess:\n    init.run() #initializes the variables\n    for epoch in range n_outputs:\n        for iteration in range (m // batch_size):\n            x_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: x_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: x_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict = {})\n        \n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# DNNClassifier ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nprint(x_train.shape)\nprint(y_train.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "x_train = x_train.reshape(60000, 784)\n#y_train = pd.get_dummies(y_train)\nprint(x_train.shape)\nprint(y_train.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "from sklearn.preprocessing import MinMaxScaler", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "scalar = MinMaxScaler(feature_range=(0, 1))\nscalar.fit(x_train)\nx_train = scalar.transform(x_train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "y_train_int32 = np.asarray(y_train, dtype=np.int32)\nprint(y_train_int32.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "print(x_train.shape)\nprint(y_train.shape)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(x_train)\ndnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=feature_cols)\ndnn_clf = tf.contrib.learn.SKCompat(dnn_clf)\ndnn_clf.fit(x_train, y_train, batch_size = 50, steps = 40000)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "feature_cols", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def foo():\n  with tf.variable_scope(\"foo\", reuse=True):\n    v = tf.get_variable(\"v\", [1])\n  return v", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "v1 = foo()  # Creates v.\nv2 = foo()  # Gets the same, existing v.\nprint(v1 == v2)\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    init.run()\n    print(v1.eval())\n    print(v1.assign(2.0, validate_shape=False))\n    print(v1.eval())\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}